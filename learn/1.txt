 Â§ 7.4 Pipe and Pipeline Methods

 ðŸ”¹ `stream.pipe()`

 The simplest way to connect streams.
 It takes the output of a readable stream and passes it into a writable stream automatically.
 Handles backpressure (pauses reading if writing is slower).

Example: Copy a file using `pipe()`

```js
const fs = require('fs');

// Create readable & writable streams
const readStream = fs.createReadStream('input.txt');
const writeStream = fs.createWriteStream('output.txt');

// Pipe data directly
readStream.pipe(writeStream);
```

ðŸ‘‰ Equivalent to reading chunks manually and writing them, but much cleaner.

---

 ðŸ”¹ `stream.pipeline()`

 Introduced in Node.js v10.
 Like `pipe()`, but with better error handling.
 Ensures that if any stream in the chain fails, the whole pipeline ends gracefully.

Example: Compress a file with pipeline

```js
const fs = require('fs');
const zlib = require('zlib');
const { pipeline } = require('stream');

pipeline(
  fs.createReadStream('input.txt'),
  zlib.createGzip(),                   // transform stream
  fs.createWriteStream('input.txt.gz'),
  (err) => {
    if (err) {
      console.error('Pipeline failed:', err);
    } else {
      console.log('Pipeline completed successfully');
    }
  }
);
```

âœ… When to use what?

 Use `pipe()` for simple cases.
 Use `pipeline()` for production code where error handling is critical.

---

 Â§ 7.5 Stream Events and Error Handling

Streams are built on EventEmitter, so they emit events.

 ðŸ”¹ Common Events

 For Readable Streams

 `data` â†’ emitted when a chunk of data is available
 `end` â†’ no more data left
 `error` â†’ an error occurred
 `close` â†’ stream is closed

```js
const fs = require('fs');
const readStream = fs.createReadStream('input.txt', 'utf8');

readStream.on('data', (chunk) => {
  console.log('Received chunk:', chunk.length);
});

readStream.on('end', () => {
  console.log('Reading finished');
});

readStream.on('error', (err) => {
  console.error('Error while reading:', err);
});
```

 For Writable Streams

 `drain` â†’ buffer is empty, safe to write more
 `finish` â†’ `.end()` has been called and all data flushed
 `error` â†’ error while writing

```js
const writeStream = fs.createWriteStream('output.txt');
writeStream.on('finish', () => {
  console.log('Writing completed');
});
writeStream.on('error', (err) => {
  console.error('Error while writing:', err);
});
```

âœ… Error Handling Best Practice

 Always add `.on('error')` for both readable and writable streams.
 With `pipeline()`, error handling is automatic.

---

 Â§ 7.6 Creating Custom Streams

Sometimes you need a custom stream (not just file read/write).
Node.js provides `stream` module classes to extend:

 `Readable` â†’ to create your own readable data source
 `Writable` â†’ to create your own writable sink
 `Duplex` â†’ to both read & write
 `Transform` â†’ to modify data

---

 ðŸ”¹ Custom Readable Stream

```js
const { Readable } = require('stream');

class MyReadable extends Readable {
  constructor(data, options) {
    super(options);
    this.data = data;
  }

  _read() {
    if (this.data.length === 0) {
      this.push(null); // No more data
    } else {
      this.push(this.data.shift()); // Send next chunk
    }
  }
}

// Example usage
const myStream = new MyReadable(['Hello ', 'World!', '\n']);
myStream.pipe(process.stdout); // prints: Hello World!
```

---

 ðŸ”¹ Custom Writable Stream

```js
const { Writable } = require('stream');

class MyWritable extends Writable {
  _write(chunk, encoding, callback) {
    console.log('Received:', chunk.toString());
    callback();
  }
}

// Example usage
const myWritable = new MyWritable();
myWritable.write('Hi there!\n');
myWritable.end();
```

---

 ðŸ”¹ Custom Transform Stream

```js
const { Transform } = require('stream');

class UpperCaseTransform extends Transform {
  _transform(chunk, encoding, callback) {
    // Convert chunk to uppercase
    this.push(chunk.toString().toUpperCase());
    callback();
  }
}

// Example usage
process.stdin
  .pipe(new UpperCaseTransform())
  .pipe(process.stdout);
```

ðŸ‘‰ Type something in the terminal â†’ it outputs in UPPERCASE.

---

âœ… Summary

 Pipe vs Pipeline â†’ Both connect streams, but `pipeline()` has better error handling.
 Stream Events â†’ Handle `data`, `end`, `finish`, `error` for smooth flow.
 Custom Streams â†’ Extend `Readable`, `Writable`, `Duplex`, or `Transform` to build your own streaming logic.

---

Would you like me to also prepare a real-world mini project (like a file compressor with error handling + custom logging stream) that combines pipe, events, and custom streams?
